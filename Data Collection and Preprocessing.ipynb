{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41311e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect Data from snscrape to \n",
    "try:\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import snscrape.modules.twitter as sntwitter\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    print(\"Some Modules are Missing \",e)\n",
    "\n",
    "class tweetCollection(object):\n",
    "    query = 'ChatGPT since:2022-12-01 until:2023-04-11 , lang:en'\n",
    "    limit = 10000\n",
    "    tweets_1 = []\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        if len(tweets_1) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets_1.append([tweet.id,\n",
    "                             tweet.user.username,\n",
    "                             tweet.user.verified,\n",
    "                             tweet.user.location,\n",
    "                             tweet.user.followersCount,\n",
    "                             tweet.rawContent,\n",
    "                             tweet.date,\n",
    "                             tweet.retweetedTweet,\n",
    "                             tweet.lang])\n",
    "    df_1 = pd.DataFrame(tweets_1, columns=['UserId',\n",
    "                                           'UserName',\n",
    "                                           'Verified',\n",
    "                                           'Location',\n",
    "                                           'Followers',\n",
    "                                           'Tweet',\n",
    "                                           'Date',\n",
    "                                           'Retweeted',\n",
    "                                           'Language'])\n",
    "    df_1.to_csv('Twitter_Data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tweetCollection()\n",
    "    myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    mydb = myclient[\"mydb\"]\n",
    "    mycol = mydb[\"mycollection\"]\n",
    "    df=pd.read_csv(\"Twitter_Data.csv\",low_memory=False)\n",
    "    data=df.to_dict(orient=\"records\")\n",
    "    mycol.insert_many(data)\n",
    "    print(\"Inserted Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get Data from MongoDB to csv file\n",
    "class MongoDB(object):\n",
    "\n",
    "    def __init__(self, dBName=None, collectionName=None):\n",
    "\n",
    "        self.dBName = dBName\n",
    "        self.collectionName = collectionName\n",
    "        self.client = MongoClient(\"localhost\", 27017, maxPoolSize=50)\n",
    "        self.DB = self.client[self.dBName]\n",
    "        self.collection = self.DB[self.collectionName]\n",
    "\n",
    "    def dbFetch(self, dBName=None, collectionName=None):\n",
    "        myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        mydb = myclient[\"mydb\"]\n",
    "        mycol = mydb[\"mycollection\"]\n",
    "        x = mycol.find()\n",
    "        tweets_1 = []\n",
    "        for tweet in x:\n",
    "            tweets_1.append([tweet['UserId'],\n",
    "                             tweet['UserName'],\n",
    "                             tweet['Verified'],\n",
    "                             tweet['Location'],\n",
    "                             tweet['Followers'],\n",
    "                             tweet['Tweet'],\n",
    "                             tweet['Timestamp'],\n",
    "                             tweet['Retweeted'],\n",
    "                             tweet['Language']])\n",
    "            \n",
    "            \n",
    "            df_1 = pd.DataFrame(tweets_1, columns=['UserId',\n",
    "                                           'UserName',\n",
    "                                           'Verified',\n",
    "                                           'Location',\n",
    "                                           'Followers',\n",
    "                                           'Tweet',\n",
    "                                           'Date',\n",
    "                                           'Retweeted',\n",
    "                                           'Language'])\n",
    "            df_1.to_csv('Twitter_Data.csv', index=False)\n",
    "            print(tweet)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mongodb = MongoDB(dBName = 'mydb', collectionName='mycollection')\n",
    "    mongodb.dbFetch()\n",
    "    print(\"Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import snscrape.modules.twitter as sntwitter\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    print(\"Some Modules are Missing \",e)\n",
    "    \n",
    "#Read data from csv stored from Mongodb\n",
    "df_tweets = pd.read_csv('Twitter_Data.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb78f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.drop_duplicates()\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the datetime columns into two separate columns\n",
    "df_tweets['Timestamp']=pd.to_datetime(df_tweets['Date'])\n",
    "df_tweets['Date'] = df_tweets['Timestamp'].dt.date\n",
    "df_tweets['Time_of_the_day'] = df_tweets['Timestamp'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f47368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[df_tweets['Timestamp'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove missing values\n",
    "df_tweets.dropna(subset=['Timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39795636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for EDA \n",
    "df_tweets.to_csv('Twitter_EDA.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "except Exception as e:\n",
    "    print(\"Some Modules are Missing \",e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe212860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df_tweets[df_tweets['Language']=='en'].reset_index(drop=True)\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df_en.drop(columns = ['Language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URL from tweet text\n",
    "df_en['Tweet'] = df_en['Tweet'].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
    "#Remove mention (@user)\n",
    "df_en['Tweet'] = df_en['Tweet'].apply(lambda x: re.sub(r'@\\w+', '',x))\n",
    "#All lowercases\n",
    "df_en['Tweet'] = df_en['Tweet'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "#Remove Punctuation\n",
    "df_en['Tweet_punc'] = df_en['Tweet'].apply(lambda x: re.sub('[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 'chat gpt' with 'chatgpt'\n",
    "df_en['Tweet_punc'] = df_en['Tweet_punc'].apply(lambda x: re.sub(r'chat gpt', 'chatgpt', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae277705",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "df_en['Tweet_stop'] = df_en['Tweet_punc'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['Tweet_tokenized'] = df_en['Tweet_stop'].apply(lambda x: re.split('\\W+', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7382ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to map POS tags from treebank tag into wordnet tags\n",
    "wnl = WordNetLemmatizer()\n",
    "def get_wordnet(pos):\n",
    "    if pos.startswith('N'): \n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN     #default to noun if no match\n",
    "\n",
    "    \n",
    "# Define a function to tag and lemmatize a text string    \n",
    "def lemmatizer(text):\n",
    "    pos_tags = pos_tag(text)\n",
    "    text = [wnl.lemmatize(word, pos = get_wordnet(pos)) for word, pos in pos_tags]\n",
    "    return text\n",
    "\n",
    "df_en['Tweet_lemmatized'] = df_en['Tweet_tokenized'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install --upgrade dataclasses\n",
    "#import spacy\n",
    "# try spacy lemmatizer\n",
    "###\n",
    "def spacy_lemmatizer(text):\n",
    "    # only need tagger, no need for parser and named entity recognizer, for faster implementation\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    doc = nlp(' '.join(text))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "#df_en['Tweet_lemmatized_spacy'] = df_en['Tweet_tokenized'].apply(lambda x: spacy_lemmatizer(x))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064453ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns not needed.\n",
    "df_en_clean = df_en.drop(columns=['Tweet', 'Tweet_punc', 'Tweet_stop', 'Tweet_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_clean['Tokens'] = df_en_clean['Tweet_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_clean.to_csv('Twitter_eng_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597d289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a18d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
